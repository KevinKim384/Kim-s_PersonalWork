{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e939a36d-efc9-4e82-a872-77f48e87f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용환경을 준비! 참고로 저는 환경변수에 넣었기 때문에(공유가 아니라 제가 이미 돈을 넣어서 제 계정으로 된 API를 쓰고 있습니다!)\n",
    "#os.getenv를 통해 제 환경변수에 있는 API키 가져오기! 그리고 불필요한 경고문 제거를 위해 warnings 추가\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "698dcdab-0d85-488d-a76b-73b1b37562ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "client = ChatOpenAI(model = 'gpt-4o', temperature = 0.5, max_tokens = 2000)\n",
    "#PyPDFLoader를 통해 pdf 파일 모델 불러오기!\n",
    "loader = PyPDFLoader('C:\\\\Users\\\\kevinkim\\\\OneDrive\\\\바탕 화면\\\\Sparta_Work\\\\초거대 언어모델 연구 동향.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95c2a162-4cfc-4d16-adee-50374676bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#요건 RecursiveCharacterTextSplitter: CharacterTextSplitter와 유사하지만, 더 복잡한 분할 방식을 제공\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #chunk_size: 각 청크의 최대 길이\n",
    "    chunk_size=500,\n",
    "    #chunk_overlap: 청크 간 겹침의 길이\n",
    "    chunk_overlap=10,\n",
    "    #lenth_function: 텍스트 청크의 길이를 계산하는 데 사용하는 함수(기본적으로 len 함수 사용)\n",
    "    length_function=len,\n",
    "    #is_separator_regex: 지정한 구분자가 정규식인지 여부를 지정하는 플래그\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "splitted_docs = recursive_text_splitter.split_documents(docs)\n",
    "\n",
    "#이것은 CharacterTextSplitter: 텍스트를 일정한 길이나 규칙에 따라 분할하는 단순한 방법을 제공. \n",
    "#CharacterTextSplitter와 비슷한 방식으로 텍스트를 나누고, \n",
    "#나누어진 조각이 여전히 너무 길면 다시 재귀적으로 분할을 시도\n",
    "#text_splitter = CharacterTextSplitter(\n",
    "    #separator=\"\\n\\n\", -> 텍스트 분할 시 사용할 구분자\n",
    "    #chunk_size=100, -> 설명 완료\n",
    "    #chunk_overlap=10,-> 설명 완료\n",
    "    #length_function=len,-> 설명 완료\n",
    "    #is_separator_regex=False,-> 설명 완료\n",
    "#)\n",
    "#splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a8b3607-45f8-452f-b11d-a94614d85dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#너무 길어서 여기는 주석처리로... enumerate을 통해 index와 각 청크를 따로 분리 후, \n",
    "#for i, splitted in enumerate(splitted_docs[:50]):\n",
    "    #print(f'Chunk {i+1}:')\n",
    "    #print(splitted)\n",
    "    #print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0f7cf85-f9a8-4a19-bcb0-72e47c3e2647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c5cdbd54-d3ed-4802-8206-369679afda83',\n",
       " '744f07b3-125d-4d95-aef5-bd116035f1e5',\n",
       " '4d3403e3-ca9f-420a-bc49-36e610070a12',\n",
       " 'e7a98169-5b54-4d70-8275-d0c542b5bab8',\n",
       " 'fd11c87d-4971-4e42-9c27-6237e556679a',\n",
       " 'fbc03635-d981-4def-a61d-da5ebc8dfe9c',\n",
       " 'ff7abbdb-a76b-4db2-aaaa-d4771d1c8939',\n",
       " 'c96c6b91-987b-467b-bee5-05c558c4eac6',\n",
       " 'a8e1ce66-dd0d-47ac-bd64-e8c2e5ec92d9',\n",
       " '463b0d9e-7adb-4992-a292-3e77ae9645bd',\n",
       " 'f0c3a1de-3a42-4427-8b83-7898efd81be4',\n",
       " 'dc71f5a2-bc83-4897-ba74-ebe0b612fc6f',\n",
       " 'f0b902ed-2eaf-4441-9075-2374278fb1cc',\n",
       " 'bebad21d-34c6-427f-a0e7-8292cbfa2432',\n",
       " 'fc3d8e0f-9c2a-41a7-96ed-1a433ec6db93',\n",
       " '6ca6ee32-eaec-421f-842d-57528d6c4348',\n",
       " 'e428bbd2-3797-42e2-8e27-8e49ca27b6b7',\n",
       " '7bbaea5b-3072-49c8-b33b-93431283b955',\n",
       " '79091318-d0b7-4908-8c55-3fa61d392eff',\n",
       " 'a06d0c63-66f8-4276-95a6-39b49a612350',\n",
       " '883d8d18-5826-44b1-aba2-a47017f11557',\n",
       " 'de536e12-6bbe-4103-9355-59530c5cbc62',\n",
       " '5cddfafa-2fcb-463b-a331-56ffc118ef6e',\n",
       " '81b8a0f5-d516-4144-bd83-1e56aac47282',\n",
       " '75925064-3d76-45fe-91e2-a32b0a084e9f',\n",
       " '4ba289a5-7ee4-446d-a105-88624d9bbc96',\n",
       " 'c58a6d22-2559-49f3-a422-3caf6090cefb',\n",
       " '64758245-2345-4b0a-8f0c-dcaa4c26b8b5',\n",
       " '61d76dc3-df2a-48ea-88dd-deba3958e054',\n",
       " '5ca7c38c-19ec-48ab-ab97-347991a52f19',\n",
       " 'e453c412-17ab-4df2-9808-1d3f914e3dde',\n",
       " '87dc909a-5d28-455e-9341-6dde8616b201',\n",
       " '5df405c9-86c2-4030-8070-3d730936cca5',\n",
       " 'a42d1d17-847d-447a-9ead-05a493aa3f4b',\n",
       " 'd464edfb-a679-4275-8ccc-f1d784ad8254',\n",
       " '6ac998f9-9f58-4554-b4fa-96d5e48b54dd',\n",
       " '6a1f3498-f44f-4505-bcaf-12cb86eac9b0',\n",
       " '5a82f1cc-49c7-4d16-84ba-0d4da755a699',\n",
       " 'c8aa4d28-7820-48e0-89c0-ebdf3427baba',\n",
       " '516af792-d64b-4a41-80a1-2fdbe0639657',\n",
       " 'dcd2c1f5-56d4-44f8-af56-305bc6ba442b',\n",
       " '46920262-8a60-4278-9252-a16c31db9b14',\n",
       " 'ba49fa3e-a666-4791-b978-6aae13321d4f',\n",
       " '85f95f12-a79a-40cf-b6e7-92b6a82e5d8c',\n",
       " '5e5eddbb-1e29-47b4-80b4-c7fbd6109596',\n",
       " '81703147-f78f-4def-b8fc-a36abc35d748',\n",
       " 'c5b4b0a6-54fe-4619-99f0-d8135fbd4f6b',\n",
       " '3fac672d-0406-41c1-b932-c30f4c466a5d',\n",
       " '82b45654-56cb-44a0-9f21-cb89325986d5',\n",
       " 'd36a7dbc-72c5-4e9c-9e64-3508788f48df',\n",
       " 'f76033f5-d45b-4440-bd47-186d2353ffe1',\n",
       " 'cc2abe9b-da6a-42de-a7eb-73bd41702285',\n",
       " '7b4e23af-fb88-4cca-afa3-5d0f4b78cb16',\n",
       " 'c39958bf-4747-4d51-8994-b24f1df196d7',\n",
       " '28c80f98-b4e5-4683-9158-c23a1fa69102',\n",
       " '992987eb-eb5d-41e2-ad6c-49ea6f45d2c2',\n",
       " '05f05a9f-81e9-43b6-a894-5c67bce45cfa',\n",
       " '6c10180b-0c07-4aa6-8f87-60697171fcb3',\n",
       " '65449102-4c81-4fec-891e-7d20782abe15',\n",
       " '0c2c5171-4659-4569-af07-ad62b9fba038',\n",
       " 'efad8aa3-5779-479c-89bf-7d394978be0d',\n",
       " 'ef4fb3b8-ca7a-475e-954d-1000ff2bf627',\n",
       " '61fe39fa-56b2-4a25-a7cb-ec601e1d2793',\n",
       " '655dc995-06b7-444b-b8de-e9788f6eec0e',\n",
       " 'a8274cba-a0b0-4a9f-97fe-3007fdbc4065',\n",
       " '4b2c710d-6472-46d9-98d6-59f8bd7b73cf',\n",
       " '4609b62a-4d8a-4700-9eea-77ebc6202f07',\n",
       " 'e519be0a-4d70-4e4e-a787-b51b62588e60',\n",
       " '7119a4ae-64df-428a-ab82-6d05e62c5684',\n",
       " '4f75357b-c95a-484f-8b31-f34040704a03',\n",
       " 'ee49c491-9a94-4b1f-9dad-8653d0a177cd',\n",
       " '5010daaf-0478-45c5-8ee6-3670a2d9fdf7',\n",
       " '795049d7-9e25-41f7-93a0-fcc3d027faff',\n",
       " '44bdb8ff-9c09-4b51-936a-8db18d84111d',\n",
       " '8eef7152-652b-415e-a9c1-5fba602f7f41',\n",
       " 'd953c2bd-7d43-46b6-9c06-1ca7b243e13b',\n",
       " '5aa9f1d6-3e04-43eb-ac2c-a3f649b8e292',\n",
       " '57e6f5cc-4c40-41a7-bd21-216e33b3928a',\n",
       " 'bff7d870-9a33-4b74-bb6f-9d7919dabcdf',\n",
       " '97f16dd7-61c1-487a-b12c-6505fed09af8',\n",
       " '536dc1c9-991a-4cd6-9d7e-3614ddfcb845',\n",
       " '519d32a5-16df-4153-acac-8983083af8f0',\n",
       " '2212d76a-414b-48b6-9a4c-0d3776358fc0',\n",
       " 'a213d662-efbe-40aa-93ef-c0a11d7d80a9',\n",
       " '968b1e0d-56cf-4080-a44b-7d5ef7529838',\n",
       " 'f687f221-6f1b-4d67-a6d7-0ede2673f937',\n",
       " '24527290-5c0e-4a30-af50-e0f3377dab1d',\n",
       " '413371d2-ea48-4f85-8e7a-29295a49180d',\n",
       " 'ec195654-b155-4184-a3e2-496a0b0bbf70',\n",
       " '05d96214-2a72-4f60-8d17-1887b1b2e9f6',\n",
       " '073636b9-4e15-4595-9641-52a233f85a25',\n",
       " 'f129f109-24d5-4dd5-93ec-fc8ce27b45dc',\n",
       " '91abbacd-3659-4695-8364-b6f3a6557f29',\n",
       " '6ddb18b8-20af-4a8e-a79c-7c4ffe1df523',\n",
       " '3ed0fc95-5075-405f-88ab-a87e99064b39',\n",
       " '0bf23740-ee63-4a45-84d7-5a162e793b84',\n",
       " '9239e477-2c80-4d49-ae72-82adeeece1c9',\n",
       " '26ccc153-636f-4d72-a58c-7ca0a859d8ef',\n",
       " '0c36f2e2-a38d-4842-91d1-6b696806d16e',\n",
       " 'bbe66157-6a06-433a-b020-6b21bd6b1606',\n",
       " '1c801e5e-ea95-406c-9ca9-c3dbe4bef6ed',\n",
       " 'ba943c55-7e35-4a91-9745-8b141c6829e6',\n",
       " 'c192d2c6-ed42-41c0-a54e-d0ebc86e440b',\n",
       " 'ca03d950-8a78-486a-bda1-3de71b7a3af0',\n",
       " 'eb193737-0976-4804-af2f-17441613dfe6',\n",
       " '461d9a99-02c7-484b-bc45-40a0da7c553c',\n",
       " '978dda2d-a0b0-4f9b-ad0c-c6c1373203c1',\n",
       " '2f3d9018-3d65-4ada-9c68-bee5566064b8',\n",
       " '1365041e-3255-4d27-b14b-fccf0c833a57',\n",
       " 'bc0599ea-2430-4513-af89-a139fdd054ec',\n",
       " '0a510838-cc63-498e-8da2-a72dddcc662f',\n",
       " '8da2a59b-c652-497e-827d-4ddb688e4c6c',\n",
       " 'dde1417e-917b-4f4f-94a5-bcd202bf6cf6',\n",
       " '97eca2b5-355b-48b3-b0c4-6fb250cd2f7a',\n",
       " '76833466-7845-49c3-88f8-0b68878e38bc',\n",
       " '862b4890-755a-4e78-9fcb-950a0e6fed84',\n",
       " 'd8fa6c3a-8750-4e97-bb29-a395ded8548d',\n",
       " '307e1a7b-273a-4fbb-8ffe-75b4fa8da7ce',\n",
       " 'da364b15-36e8-41d9-b51b-32fda5ffe5fa',\n",
       " '9f11c153-5c98-4117-8cdb-92513b306972',\n",
       " '171acf41-ed0d-4335-9808-519709905fa9']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from uuid import uuid4\n",
    "import faiss\n",
    "from langchain.vectorstores import Chroma\n",
    "embeddings = OpenAIEmbeddings(model = 'text-embedding-ada-002')\n",
    "\n",
    "#from_documents를 통해 쉽게 vector_store 생성\n",
    "vector_store = Chroma.from_documents(documents = splitted_docs, embedding = embeddings)\n",
    "#심심하니 uuid4를 사용해 이미 chunk로 변한 값들을 고유한 코드로 된 값들로 다시 변경(이러면 PC가 사용하기 더 쉬워진다나 뭐라나...)\n",
    "uuids = [str(uuid4()) for _ in range(len(splitted_docs))]\n",
    "vector_store.add_documents(documents = splitted_docs, ids = uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8446ae64-119b-4a97-8e26-32a2841a46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "#여기서는 평병하게 retriever를 구성하였습니다.\n",
    "#Chroma에서 이 부분 llm = client를 통해 쉽게 LLM에 연결이 가능합니다.\n",
    "retriever = vector_store.as_retriever(search_type = 'similarity', search_kwargs = {'k':5})\n",
    "#여기서는 RetrieveralQA를 써서 질문과 답변을 받을 수 있게 만드는 함수\n",
    "qa_retriever = RetrievalQA.from_chain_type(\n",
    "    llm = client,\n",
    "    chain_type = 'refine',\n",
    "    retriever = retriever,\n",
    "    return_source_documents = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18967d49-becd-47ed-8caa-b91fd12fec8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "질문을 해주세요: 현재 초거대 언어모델 연구 동향에 대해 설명해줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevinkim\\AppData\\Local\\Temp\\ipykernel_40864\\4200508864.py:9: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_retriever(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 현재 초거대 언어모델 연구 동향은 다양한 측면에서 발전하고 있습니다. ChatGPT와 같은 초거대 언어모델의 등장은 자연언어처리(NLP) 분야에서 큰 변화를 가져왔습니다. 첫째로, 이러한 모델들은 번역, 요약, 질의응답, 형태소 분석 등 다양한 자연언어처리 작업을 하나의 모델로 처리할 수 있게 하여 태스크 수렴 현상이 발생하고 있습니다. 이는 기존에 병렬적으로 연구되던 다양한 하위 분야들이 하나의 모델로 통합되고 있다는 것을 의미합니다. 둘째로, 사용자 의도에 맞는 출력을 생성하기 위해 프롬프트를 어떻게 입력하느냐에 따라 모델의 다양한 능력이 창발하는 패러다임이 부각되고 있습니다. 셋째로, 모델의 크기와 복잡성을 계속해서 증가시키며 성능을 향상시키려는 노력이 이어지고 있습니다. 특히, 사전학습 데이터의 다양성을 강조하는 추세가 있으며, 다양한 코퍼스의 최적 혼합 비율과 필요한 데이터 양에 관한 연구가 초기 단계에 있지만 주목받고 있습니다. 예를 들어, GPT-3는 웹페이지뿐만 아니라 책과 뉴스 데이터를 활용했으며, Llama-1 65B 모델에서는 웹페이지 외에도 대화 데이터, 책 및 뉴스, 학술 데이터, 코드 데이터가 포함되어 있습니다. 넷째로, 한국어를 포함한 다양한 언어와 문화적 배경을 포괄할 수 있는 모델 개발이 활발히 진행되고 있으며, 이는 각 언어의 특수성을 반영한 모델 개발을 의미합니다. 다섯째로, 효율적인 학습 및 추론을 위한 새로운 알고리즘과 기술이 개발되고 있습니다. 마지막으로, 초거대 언어모델의 윤리적 사용과 관련된 문제를 해결하기 위한 연구도 중요하게 다루어지고 있습니다. 이러한 동향은 초거대 언어모델의 성능과 활용 가능성을 높이는 데 기여하고 있으며, 연구자 및 관련 전문가들에게 유용한 통찰과 지침을 제공하고자 합니다.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "#만약 질문을 계속 받을시에 작동할 while True: 하지만 비교를 위해 못 쓴다는...\n",
    "#query로 답을 받아, 그것을 토대로 값을 출\n",
    "#while True:\n",
    "query = input('질문을 해주세요:')\n",
    "#if query == 'exit':\n",
    "    #print('좋은 대화였습니다!')\n",
    "    #break\n",
    "result = qa_retriever(query)\n",
    "answer_a = result['result']\n",
    "source_documents = result['source_documents']\n",
    "\n",
    "    # 결과 출력\n",
    "print(\"Answer:\", answer_a)\n",
    "#하지만 QA특성상 추출할 값도 가져와야 하는데, 값을 가져오고 띄우지는 못하게 하는 코드\n",
    "logging.info(\"Source Documents: %s\", source_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0627f3f1-e7d7-4485-a158-1a19220b9026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초거대 언어 모델에 대한 관심은 급속히 증가하고 있으며, 이러한 모델들은 자연어 처리(NLP) 분야에서 혁신을 불러일으키고 있습니다. 초거대 언어 모델은 수십억 또는 수조 개의 매개변수를 가진 언어 모델로, 대량의 텍스트 데이터를 기반으로 사전 학습하여 다양한 언어 작업을 수행할 수 있습니다. 다음은 이 분야의 주요 연구 동향과 관련 모델들입니다.\n",
      "\n",
      "1. **GPT (Generative Pre-trained Transformer) 시리즈:** OpenAI에 의해 개발된 이 시리즈는 초거대 언어 모델의 발전을 이끈 대표적인 사례입니다. 다양한 버전이 있으며, 각 버전이 점점 더 큰 모델을 갖추고 더 향상된 언어 이해 및 생성 기능을 제공합니다. GPT-3 및 GPT-4는 각각 매개변수 수를 크게 늘리고, 더 많은 문맥을 이해하며, 다중 작업 수행 능력을 향상시켰습니다.\n",
      "\n",
      "2. **BERT (Bidirectional Encoder Representations from Transformers):** Google의 BERT는 양방향성을 도입하여 문맥에 대한 이해를 크게 개선한 모델입니다. 이는 비슷한 언어 모델의 개발에 큰 영향을 미쳤으며, 다양한 다운스트림 작업에서 활용되고 있습니다.\n",
      "\n",
      "3. **T5 (Text-To-Text Transfer Transformer):** Google에서 개발한 T5는 모든 NLP 작업을 텍스트를 입력하고 출력하는 방식으로 통합하여 일관된 접근 방식을 제공합니다. 이는 텍스트 생성, 번역, 요약 등 다양한 작업에 활용됩니다.\n",
      "\n",
      "4. **MosaicML, EleutherAI와 같은 오픈 소스 노선:** 이들 기관은 접근이 어려운 초거대 모델들을 대중에게 제공하는 데 중점을 두고 있습니다. 특히 EleutherAI의 GPT-Neo 및 GPT-J는 대규모 모델을 공개해 산업 및 학계에서 자유롭게 사용할 수 있도록 했습니다.\n",
      "\n",
      "5. **다양한 분야에서의 활용 증가:** 이러한 모델들은 고객 서비스, 콘텐츠 생성, 번역, 문서 요약 등 다양한 산업에 걸쳐 사용되고 있습니다. 특히, 초거대 모델의 성능 향상으로 인해 특정 산업의 특화된 언어 모델도 연구되고 있습니다.\n",
      "\n",
      "6. **모델 경량화 및 효율성 증가 시도:** 초거대 모델의 높은 계산 비용과 에너지 소비 문제 때문에, 파인튜닝을 통한 모델 경량화, 지식 증류 및 더 나은 하드웨어 사용을 통한 효율성 증가 등의 연구가 활발히 진행되고 있습니다.\n",
      "\n",
      "7. **윤리적 및 사회적 문제:** 이러한 모델들은 큰 영향력을 가지면서도 윤리적, 사회적 문제도 야기할 수 있습니다. 따라서 편향성 제거, 투명성 증대 및 책임감을 갖춘 사용을 위한 윤리적 연구도 중요합니다.\n",
      "\n",
      "이와 같은 초거대 언어 모델의 발전은 앞으로 더욱더 다양한 영역에서 혁신을 가져올 것입니다. 이와 동시에, 잠재적인 문제점들에 대한 고려가 필수적입니다.\n"
     ]
    }
   ],
   "source": [
    "#일반 LLM 모델인 GPT를 이용해 같은 질문을 받아 출력 비교\n",
    "from openai import OpenAI\n",
    "client2 = OpenAI()\n",
    "completion = client2.chat.completions.create(\n",
    "    model = 'gpt-4o',\n",
    "    messages = [\n",
    "        {'role' : 'system', 'content' : '넌 초거대 언어모델 연구 동향에 대해 설명해주는 AI야. 여러 관련 모델들과 지식들을 설명해줘.'},\n",
    "        {'role' : 'user', 'content' : query}\n",
    "    ]\n",
    ")\n",
    "answer_b = completion.choices[0].message.content\n",
    "print(answer_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de8637d8-2ead-477b-a481-b68bed15d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both assistants provided detailed explanations regarding the current research trends in large language models, each touching on various aspects of this rapidly evolving field.  \n",
      "\n",
      "Assistant A starts by discussing the impact of large language models on the convergence of tasks in natural language processing (NLP) and highlights the trend of emergent abilities depending on the input prompts. Assistant A also delves into efforts to increase model size and complexity, notes the importance of diverse pre-training data, emphasizes the development of models that encompass various languages and cultural backgrounds, and discusses new algorithms for efficient learning and inference. Additionally, Assistant A mentions the ethical considerations surrounding the use of large language models and their implications for researchers and industry experts.\n",
      "\n",
      "Assistant B begins with an overview of large language models, emphasizing the scale of parameters and their utility in NLP. It then provides specific examples of influential models including the GPT and BERT series, and describes their contributions to improving language understanding and generation. Assistant B also highlights Google's T5 model and the open-source contributions from organizations like MosaicML and EleutherAI. Furthermore, Assistant B discusses the increased application of these models across different industries, efforts to improve model efficiency, and the ethical and social issues that arise from their deployment.\n",
      "\n",
      "In terms of comparison, both answers are comprehensive but take slightly different approaches. Assistant A provides a broader discussion encompassing task convergence, emerging abilities, multilingual capabilities, and ethical research, presenting a holistic view of various facets influencing large language model research. Assistant B offers a more structured approach, detailing specific examples of key models and organizations, which adds concreteness to its response, and similarly touches on application, efficiency, and ethical issues.\n",
      "\n",
      "Considering the explained breadth and implications, depth of insight, and creative integration of specific aspects of large language models, Assistant A's response has a more nuanced treatment. It presents a forward-looking summary of diverse ongoing activities and trends, making it somewhat more informative and insightful.\n",
      "\n",
      "[[A]]\n"
     ]
    }
   ],
   "source": [
    "#A가 더 좋다는 답변을 받았다! 즉, 내가 만든 값이 더 정확하고 좋다는 뜻!\n",
    "prompt = f\"\"\"[System]\n",
    "Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
    "AI assistants to the user question displayed below. You should choose the assistant that\n",
    "follows the user's instructions and answers the user's question better. Your evaluation\n",
    "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
    "and level of detail of their responses. Begin your evaluation by comparing the two\n",
    "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
    "order in which the responses were presented does not influence your decision. Do not allow\n",
    "the length of the responses to influence your evaluation. Do not favor certain names of\n",
    "the assistants. Be as objective as possible. After providing your explanation, output your\n",
    "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
    "if assistant B is better, and \"[[C]]\" for a tie.\n",
    "\n",
    "[User Question]\n",
    "{query}\n",
    "\n",
    "[The Start of Assistant A's Answer]\n",
    "{answer_a}\n",
    "[The End of Assistant A's Answer]\n",
    "\n",
    "[The Start of Assistant B's Answer]\n",
    "{answer_b}\n",
    "[The End of Assistant B's Answer]\"\"\"\n",
    "\n",
    "completion = client2.chat.completions.create(\n",
    "    model = 'gpt-4o',\n",
    "    messages = [\n",
    "        {'role' : 'user', 'content' : prompt}\n",
    "    ]\n",
    ")\n",
    "final = completion.choices[0].message.content\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b054a5c-419a-44e6-a0c8-bd04d07f7ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG 모델은 먼저 입력된 질문과 관련된 정보를 검색하고, 그 정보를 바탕으로 답변을 생성합니다. \\n이렇게 하면 모델이 정확하고 최신의 정보를 기반으로 답변을 생성할 수 있습니다.\\n특히 Chroma와 같은 벡터 데이터베이스를 사용하여 RAG 모델을 구현할 때, 모델의 성능을 크게 향상시킬 수 있습니다.\\n특히 Chroma는 벡터 데이터베이스로, 문서나 텍스트 데이터를 벡터화하여 빠르고 효율적인 검색을 지원하는데, RAG 모델에서 Chroma와 같은 검색 시스템을 사용하면, \\n모델이 실시간으로 관련 정보를 빠르게 검색하고, 그 정보를 바탕으로 더 적합한 답변을 생성할 수 있습니다.\\n한 마디로 LLM 같은 대규모 랭귀지 모델에서 RAG를 통해 최신 정보를 토대로 정확한 답을 생성해낼 수 있기 때문입니다.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"RAG 모델은 먼저 입력된 질문과 관련된 정보를 검색하고, 그 정보를 바탕으로 답변을 생성합니다. \n",
    "이렇게 하면 모델이 정확하고 최신의 정보를 기반으로 답변을 생성할 수 있습니다.\n",
    "특히 Chroma와 같은 벡터 데이터베이스를 사용하여 RAG 모델을 구현할 때, 모델의 성능을 크게 향상시킬 수 있습니다.\n",
    "특히 Chroma는 벡터 데이터베이스로, 문서나 텍스트 데이터를 벡터화하여 빠르고 효율적인 검색을 지원하는데, RAG 모델에서 Chroma와 같은 검색 시스템을 사용하면, \n",
    "모델이 실시간으로 관련 정보를 빠르게 검색하고, 그 정보를 바탕으로 더 적합한 답변을 생성할 수 있습니다.\n",
    "한 마디로 LLM 같은 대규모 랭귀지 모델에서 RAG를 통해 최신 정보를 토대로 정확한 답을 생성해낼 수 있기 때문입니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11fb1c19-413c-4410-b6df-30d44b9f3649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 0.125, 'p': 0.18072289156626506, 'f': 0.14778324639763174}, 'rouge-2': {'r': 0.016835016835016835, 'p': 0.02577319587628866, 'f': 0.020366593998034976}, 'rouge-l': {'r': 0.12083333333333333, 'p': 0.1746987951807229, 'f': 0.1428571380232475}}]\n"
     ]
    }
   ],
   "source": [
    "#한번 해보고 싶었던..\n",
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "score = rouge.get_scores(answer_a, answer_b)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43d4a1-b232-4477-8545-c764184af9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/kevinkim/Desktop/prompts\\prompt1.txt\n",
      "결과 저장 완료: C:/Users/kevinkim/Desktop/results\\prompt1_20241119_161902.txt\n",
      "C:/Users/kevinkim/Desktop/prompts\\prompt2.txt\n",
      "결과 저장 완료: C:/Users/kevinkim/Desktop/results\\prompt2_20241119_161956.txt\n",
      "C:/Users/kevinkim/Desktop/prompts\\prompt3.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "# 디렉토리 경로\n",
    "PROMPT_DIR = r'C:/Users/kevinkim/Desktop/prompts'\n",
    "RESULT_DIR = r'C:/Users/kevinkim/Desktop/results'\n",
    "\n",
    "\n",
    "# 디렉토리 생성\n",
    "os.makedirs(PROMPT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "def load_prompt(prompt_name):\n",
    "    \"\"\"프롬프트 파일 읽기\"\"\"\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_name)\n",
    "    print(prompt_path)\n",
    "    if not os.path.exists(prompt_path):\n",
    "        raise FileNotFoundError(f\"프롬프트 파일 {prompt_name}이(가) 존재하지 않습니다.\")\n",
    "    \n",
    "    with open(prompt_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def save_result(prompt_name, result):\n",
    "    \"\"\"결과를 파일로 저장\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    result_file_name = f\"{os.path.splitext(prompt_name)[0]}_{timestamp}.txt\" #prompt1/ .txt -> [0]\n",
    "    result_path = os.path.join(RESULT_DIR, result_file_name)\n",
    "    \n",
    "    with open(result_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(str(result))\n",
    "    print(f\"결과 저장 완료: {result_path}\")\n",
    "\n",
    "def execute_prompt(prompt_name):\n",
    "    \"\"\"프롬프트 실행 및 결과 저장\"\"\"\n",
    "    prompt = load_prompt(prompt_name) \n",
    "    try:\n",
    "        tapping = '=' * 50\n",
    "        response = qa_retriever(prompt)\n",
    "        # 결과 문자열 생성\n",
    "        result = (\n",
    "            f\"{tapping}\\n\"\n",
    "            f\"Prompt:\\n{prompt}\\n\\n\"\n",
    "            f\"Response:\\n{response['result']}\\n\\n\"\n",
    "            f\"{tapping}\"\n",
    "        )\n",
    "        save_result(prompt_name, result)\n",
    "    except Exception as e:\n",
    "        print(f\"에러 발생: {e}\")\n",
    "\n",
    "files = [f for f in os.listdir(PROMPT_DIR) if f.endswith('.txt')]\n",
    "for x in files:\n",
    "    execute_prompt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a7b827f-52e3-49bb-8ef8-f9883c6cd31b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQA Retriever Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response_text' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"QA Retriever Response: {response_text}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb029e79-b915-48f5-a8ce-1b058579cb33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
